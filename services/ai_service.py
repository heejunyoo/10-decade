from services.config import config
from services.logger import get_logger

logger = get_logger("ai_service")

class AIService:
    def generate_time_capsule_question(self, author_name: str, context: dict = None) -> str:
        """
        Generates a deep, reflective question for the Time Capsule.
        """
        provider = config.get("ai_provider")
        logger.info(f"Generating Time Capsule Question via {provider} for {author_name}")

        # 1. Context Analysis
        if context:
            date_str = context.get('date', 'ì•Œ ìˆ˜ ì—†ëŠ” ë‚ ì§œ')
            location = context.get('location', 'ì–´ë–¤ ì¥ì†Œ')
            people_list = context.get('people', [])
            people_str = ", ".join(people_list) if people_list else "ì†Œì¤‘í•œ ì‚¬ëŒ"
            caption = context.get('caption', 'ì‚¬ì§„ ì„¤ëª… ì—†ìŒ')
            
            user_prompt = (
                f"ë‹¤ìŒì€ íƒ€ì„ìº¡ìŠì— ë‹´ê¸¸ ì‚¬ì§„ì˜ ì •ë³´ì…ë‹ˆë‹¤:\n"
                f"- ì´¬ì˜ ë‚ ì§œ: {date_str}\n"
                f"- ì¥ì†Œ: {location}\n"
                f"- í•¨ê»˜ ìˆëŠ” ì‚¬ëŒ: {people_str}\n"
                f"- ì‹œê°ì  ë¬˜ì‚¬: {caption}\n\n"
                f"ì‘ì„±ì({author_name})ê°€ 10ë…„ ë’¤ ì´ ì‚¬ì§„ì„ ë‹¤ì‹œ ë´¤ì„ ë•Œ, ë‹¹ì‹œì˜ ê°ì •ì„ ë– ì˜¬ë¦´ ìˆ˜ ìˆëŠ” ì§ˆë¬¸ì„ í•˜ë‚˜ë§Œ ì‘ì„±í•´ ì£¼ì„¸ìš”."
            )
        else:
            # Fallback: No Context (Generic Mode)
            user_prompt = f"ì‘ì„±ì({author_name})ê°€ 10ë…„ ë’¤ì˜ ìì‹ ì´ë‚˜ ê°€ì¡±ì—ê²Œ ë‚¨ê¸¸ ë©”ì‹œì§€ë¥¼ ì“¸ ìˆ˜ ìˆë„ë¡, ê¹Šì´ ìˆëŠ” ì§ˆë¬¸ì„ í•˜ë‚˜ë§Œ ë˜ì ¸ì£¼ì„¸ìš”."

        # 2. System Prompt Engineering (Antigravity Protocol)
        system_prompt = (
            "ë‹¹ì‹ ì€ ì‚¬ìš©ìê°€ 10ë…„ ë’¤ì˜ ë¯¸ë˜ì— ì—´ì–´ë³¼ 'íƒ€ì„ìº¡ìŠ'ì— ë„£ì„ ë©”ì‹œì§€ë¥¼ ì“°ë„ë¡ ë•ëŠ” **íšŒê³ ë¡ ê°€ì´ë“œ**ì…ë‹ˆë‹¤.\n"
            "ëª©í‘œ: ì‚¬ìš©ìê°€ í˜„ì¬ì˜ í–‰ë³µ, ì‚¬ë‘, ë‹¤ì§ì„ ë¯¸ë˜ë¡œ ë³´ë‚¼ ìˆ˜ ìˆë„ë¡ ìœ ë„í•˜ëŠ” ì§ˆë¬¸ì„ 1ê°œ ìƒì„±í•˜ì„¸ìš”.\n\n"
            "ğŸš« **Strict Constraints (ì ˆëŒ€ ê¸ˆì§€ì‚¬í•­):**\n"
            "1. **ì—†ëŠ” ì¸ë¬¼ ì°½ì¡° ê¸ˆì§€**: ì…ë ¥ëœ 'í•¨ê»˜ ìˆëŠ” ì‚¬ëŒ' ì™¸ì˜ ì¸ë¬¼(ì´ëª¨, ì‚¼ì´Œ, í• ë¨¸ë‹ˆ ë“±)ì„ ì ˆëŒ€ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”. ëª¨ë¥´ë©´ 'ì´ ë¶„', 'í•¨ê»˜ ìˆëŠ” ì‚¬ëŒ'ì´ë¼ê³  ì§€ì¹­í•˜ì„¸ìš”.\n"
            "2. **ë¯¸ë˜ ì‹œì  í˜¼ë™ ê¸ˆì§€**: '10ë…„ í›„ì— ë‚˜ë¥¼ ë§Œë‚˜ë©´' ê°™ì€ ê¸°ê´´í•œ í‘œí˜„ ê¸ˆì§€. '10ë…„ ë’¤ ì´ ê¸€ì„ ì½ì„ ë•Œ~'ê°€ ë§ìŠµë‹ˆë‹¤.\n"
            "3. **í‰ê°€ ê¸ˆì§€**: 'ì„±í’ˆì´ ì–´ë–¤ê°€ìš”?' ëŒ€ì‹  'ì–´ë–¤ ì ì´ ì¢‹ì•˜ë‚˜ìš”?'ì²˜ëŸ¼ êµ¬ì²´ì ì¸ ê²½í—˜/ê°ì •ì„ ë¬¼ì–´ë³´ì„¸ìš”.\n"
            "4. **ì–¸ì–´**: ë¬´ì¡°ê±´ í•œêµ­ì–´(Korean). ì •ì¤‘í•˜ê³  ë”°ëœ»í•œ ì–´ì¡°(í•´ìš”ì²´).\n"
        )

        try:
            if provider == "gemini":
                from services.gemini import gemini_service
                response = gemini_service.chat_query(system_prompt, user_prompt, temperature=0.8)
                return response.strip('" ')
            else:
                # Local Llama (Ollama)
                from services.ollama_manager import ollama_manager
                import ollama
                
                if not ollama_manager.ensure_running():
                    return "ë¯¸ë˜ì˜ ë‚˜ì—ê²Œ ì–´ë–¤ ë§ì„ ë‚¨ê¸°ê³  ì‹¶ìœ¼ì‹ ê°€ìš”? (AI ì—°ê²° ì‹¤íŒ¨)"

                model_name = ollama_manager.get_best_model()
                response = ollama.chat(
                    model=model_name, 
                    messages=[
                        {'role': 'system', 'content': system_prompt},
                        {'role': 'user', 'content': user_prompt}
                    ],
                    options={"temperature": 0.8}
                )
                return response['message']['content'].strip('" ')

        except Exception as e:
            logger.error(f"Failed to generate capsule question: {e}")
            return "1ë…„ ë’¤ì˜ ë‚˜ì—ê²Œ ì–´ë–¤ ì´ì•¼ê¸°ë¥¼ ë“¤ë ¤ì£¼ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?" 

    def generate_interview_question(self, context: dict) -> str:
        """
        Generates a context-aware interview question for a photo.
        Context keys: date, location, people (list), caption (optional)
        """
        provider = config.get("ai_provider")
        
        # Build Context String (Injection)
        # Build Context String (Injection)
        location = context.get('location', 'ì•Œ ìˆ˜ ì—†ëŠ” ì¥ì†Œ')
        date_str = context.get('date', 'ì–´ëŠ ë‚ ')
        # people = ", ".join(context.get('people', [])) or "í˜¼ì" # Disabled by User Request
        caption = context.get('caption', 'ì‚¬ì§„ ë‚´ìš© ì—†ìŒ')
        
        system_prompt = (
            "ë‹¹ì‹ ì€ ë”°ëœ»í•œ í˜¸ê¸°ì‹¬ì„ ê°€ì§„ **ê°€ì¡± ì „ê¸° ì‘ê°€(Family Biographer)**ì…ë‹ˆë‹¤. "
            "ì‚¬ìš©ìê°€ ì´ ì‚¬ì§„ì— ë‹´ê¸´ ë¹„í•˜ì¸ë“œ ìŠ¤í† ë¦¬ë¥¼ ê¸°ë¡í•˜ë„ë¡ ìœ ë„í•´ì•¼ í•©ë‹ˆë‹¤. "
            "ë‹¨ìˆœíˆ 'ì´ê±´ ë­”ê°€ìš”?'ë¼ê³  ë¬»ì§€ ë§ê³ , ì œê³µëœ ì •ë³´(ë‚ ì§œ, ì¥ì†Œ, ìƒí™©)ë¥¼ í™œìš©í•´ êµ¬ì²´ì ì¸ ë””í…Œì¼ì„ ì½• ì§‘ì–´ì„œ ì§ˆë¬¸í•˜ì„¸ìš”.\n\n"
            "ì§€ì¹¨:\n"
            "1. ì–¸ì–´: **ë¬´ì¡°ê±´ í•œêµ­ì–´(Korean)**. ë§íˆ¬ëŠ” ë‹¤ì •í•˜ê³  ì˜ˆì˜ ë°”ë¥´ê²Œ(í•´ìš”ì²´).\n"
            "2. ì§ˆë¬¸ ë°©ì‹: 'ê·¸ë‚  ~í•  ë•Œ ì–´ë–¤ ê¸°ë¶„ì´ ë“œì…¨ë‚˜ìš”?' í˜¹ì€ '~ ì¥ì†Œì˜ ë¶„ìœ„ê¸°ëŠ” ì–´ë• ë‚˜ìš”?' ì²˜ëŸ¼ êµ¬ì²´ì ìœ¼ë¡œ.\n"
            "3. **ğŸš« ì¸ë¬¼ ì´ë¦„ ì‚¬ìš© ê¸ˆì§€**: ì‚¬ì§„ ì† ì¸ë¬¼ì˜ êµ¬ì²´ì ì¸ ì´ë¦„(ì˜ˆ: Dad, ëˆ„êµ¬ë‹˜ ë“±)ì„ ì ˆëŒ€ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”. ëŒ€ì‹  'í•¨ê»˜í•œ ë¶„ë“¤', 'ê°€ì¡±ë¶„ë“¤', 'ì•„ì´' ë“±ìœ¼ë¡œ ì§€ì¹­í•˜ì„¸ìš”.\n"
            "4. ì…ë ¥ ì •ë³´ í™œìš©: ì œê³µëœ ë‚ ì§œ(ê³„ì ˆ), ì¥ì†Œ, ì‚¬ì§„ ì„¤ëª…ì„ ì§ˆë¬¸ì— ë…¹ì—¬ë‚´ì„¸ìš”.\n"
            "5. ê¸¸ì´: í•œ ë¬¸ì¥ ë˜ëŠ” ë‘ ë¬¸ì¥ìœ¼ë¡œ ì§§ê²Œ.\n"
        )
        
        user_prompt = (
            f"ë‹¤ìŒì€ ì‚¬ì§„ì— ëŒ€í•œ ì •ë³´ì…ë‹ˆë‹¤:\n"
            f"- Date: {date_str}\n"
            f"- Location: {location}\n"
            f"- Visual Description: {caption}\n\n"
            "ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìê°€ ì¶”ì–µì„ ë– ì˜¬ë¦´ ìˆ˜ ìˆëŠ” ê°ì„±ì ì¸ ì§ˆë¬¸ì„ í•˜ë‚˜ë§Œ ë§Œë“¤ì–´ì£¼ì„¸ìš”. (ì¸ë¬¼ ì´ë¦„ ì–¸ê¸‰ ê¸ˆì§€)"
        )

        logger.info(f"Generating Interview Question via {provider} with Context: {date_str}, {location}")
        
        try:
            if provider == "gemini":
                from services.gemini import gemini_service
                response = gemini_service.chat_query(system_prompt, user_prompt, temperature=0.75)
                return response.strip('" ')
            else:
                # Local Llama
                from services.ollama_manager import ollama_manager
                import ollama
                
                if not ollama_manager.ensure_running():
                    return None 

                system_prompt = (
                    "ë‹¹ì‹ ì€ ë”°ëœ»í•œ í•œêµ­ì¸ AI ë¹„ì„œì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì‚¬ì§„ ì •ë³´ë¥¼ ë³´ê³  ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ì§ˆë¬¸ì„ í•˜ë‚˜ë§Œ í•˜ì„¸ìš”.\n"
                    "ë‹¤ë¥¸ ì–¸ì–´ ì ˆëŒ€ ê¸ˆì§€. ì˜¤ì§ í•œêµ­ì–´ë§Œ ì‚¬ìš©.\n\n"
                    "ì˜ˆì‹œ:\n"
                    "ì •ë³´: ë‚ ì§œ=2023-01-01, ì„¤ëª…=ê°€ì¡± ì‹ì‚¬\n"
                    "ì§ˆë¬¸: ìƒˆí•´ ì²«ë‚  ê°€ì¡±ë“¤ê³¼ ë‚˜ëˆ„ì‹  ëŒ€í™”ê°€ ê¸°ì–µë‚˜ì‹œë‚˜ìš”?\n\n"
                    "ì •ë³´: ë‚ ì§œ=2019-08-15, ì„¤ëª…=í•´ë³€ì˜ ì•„ì´ë“¤\n"
                    "ì§ˆë¬¸: ì•„ì´ë“¤ì´ ì •ë§ ì‹ ë‚˜ ë³´ì´ë„¤ìš”! ì´ë‚  ë¬¼ë†€ì´ëŠ” ì¦ê±°ìš°ì…¨ë‚˜ìš”?\n"
                )
                
                # Simple User Prompt
                user_prompt = (
                    f"ì •ë³´: ë‚ ì§œ={date_str}, ì¥ì†Œ={location}, ì„¤ëª…={caption}\n"
                    "ì§ˆë¬¸:"
                )

                # Self-Correction Loop (Max 3 attempts)
                max_retries = 3
                for attempt in range(max_retries):
                    model_name = ollama_manager.get_best_model()
                    response = ollama.chat(
                        model=model_name, 
                        messages=[
                            {'role': 'system', 'content': system_prompt},
                            {'role': 'user', 'content': user_prompt}
                        ],
                        options={
                            "temperature": 0.7, # Increased freedom as requested
                            "repeat_penalty": 1.1, # Slightly relaxed
                            "top_p": 0.9
                        }
                    )
                    
                    response_text = response['message']['content'].strip('" ')
                    
                    # Validation
                    if self._is_contaminated(response_text):
                        logger.warning(f"âš ï¸ Attempt {attempt+1} failed (Hallucination): {response_text}. Retrying...")
                        continue # Try again
                    
                    # Success
                    return response_text

                # If all retries fail, use fallback
                logger.error("âŒ All AI attempts failed integrity check. Using fallback.")
                return self._get_fallback_question()
            
        except Exception as e:
            logger.error(f"Failed to generate interview question: {e}")
            return self._get_fallback_question()

    def _is_contaminated(self, text: str) -> bool:
        """
        Robustly checks if the text contains hallucinated scripts.
        Blocks: Japanese, Chinese, Cyrillic, Thai, Arabic, Devanagari, Greek, Hebrew.
        Allows: Korean, English (ASCII), Punctuation, Emojis.
        """
        import re
        
        # Deny-list of Unicode ranges often hallucinated by Llama-3B
        # \u3040-\u30FF: Japanese (Hiragana/Katakana)
        # \u4E00-\u9FFF: Chinese (CJK Unified Ideographs)
        # \u0400-\u04FF: Cyrillic (Russian)
        # \u0E00-\u0E7F: Thai
        # \u0600-\u06FF: Arabic
        # \u0900-\u097F: Devanagari (Hindi)
        # \u0370-\u03FF: Greek
        # \u0590-\u05FF: Hebrew
        unsafe_pattern = re.compile(r'[\u3040-\u30FF\u4E00-\u9FFF\u0400-\u04FF\u0E00-\u0E7F\u0600-\u06FF\u0900-\u097F\u0370-\u03FF\u0590-\u05FF]')
        
        # Specific token blacklists (hallucination artifacts)
        blacklist_tokens = ["-san", "-kun", "xiezhen", "shen", "chan", "desu"]
        text_lower = text.lower()
        for token in blacklist_tokens:
            if token in text_lower:
                return True
            
        return bool(unsafe_pattern.search(text))

    def _get_fallback_question(self) -> str:
        """ Returns a safe, pre-defined Korean question. """
        import random
        templates = [
            "ì´ ì‚¬ì§„ì„ ë‹¤ì‹œ ë³´ë‹ˆ ì–´ë–¤ ê¸°ë¶„ì´ ë“œì‹œë‚˜ìš”?",
            "ì´ë•Œ ê°€ì¥ ê¸°ì–µì— ë‚¨ëŠ” ì—í”¼ì†Œë“œê°€ ìˆë‹¤ë©´ ì•Œë ¤ì£¼ì„¸ìš”.",
            "ì‚¬ì§„ ì† ë¶„ìœ„ê¸°ê°€ ì°¸ ì¢‹ë„¤ìš”! ì–´ë–¤ ìƒí™©ì´ì—ˆë‚˜ìš”?",
            "í•¨ê»˜í•œ ë¶„ë“¤ê³¼ ì–´ë–¤ ì´ì•¼ê¸°ë¥¼ ë‚˜ëˆ„ì…¨ëŠ”ì§€ ê¸°ì–µë‚˜ì‹œë‚˜ìš”?",
            "ì´ ìˆœê°„ìœ¼ë¡œ ë‹¤ì‹œ ëŒì•„ê°„ë‹¤ë©´ ë¬´ì—‡ì„ í•˜ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?"
        ]
        return random.choice(templates)

    def generate_response(self, system_prompt: str, user_prompt: str, temperature: float = 0.7) -> str:
        """
        Generic method to generate text via the active AI provider.
        Supports seamless switching between Local (Ollama) and Gemini.
        """
        provider = config.get("ai_provider")
        
        try:
            if provider == "gemini":
                from services.gemini import gemini_service
                return gemini_service.chat_query(system_prompt, user_prompt, temperature)
            else:
                # Local (Ollama)
                from services.ollama_manager import ollama_manager
                import ollama
                
                if not ollama_manager.ensure_running():
                    return "ì£„ì†¡í•©ë‹ˆë‹¤. ë¡œì»¬ AI ì„œë²„(Ollama)ê°€ ì‘ë‹µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."

                model_name = ollama_manager.get_best_model()
                # logger.info(f"Generating response via Local AI: {model_name}")
                
                response = ollama.chat(
                    model=model_name, 
                    messages=[
                        {'role': 'system', 'content': system_prompt},
                        {'role': 'user', 'content': user_prompt}
                    ],
                    options={
                        "temperature": temperature,
                        "num_ctx": 4096,
                        "keep_alive": "5m" 
                    }
                )
                return response['message']['content']
                
        except Exception as e:
            logger.error(f"AI Generation Error ({provider}): {e}")
            return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

# Singleton
ai_service = AIService()
